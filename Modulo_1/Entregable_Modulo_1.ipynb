{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heNJLPGoadzh"
      },
      "source": [
        "# Entregable Modulo 1: Utilización, procesamiento y visualización de grandes volúmenes de datos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuAk9Oe0adxq"
      },
      "source": [
        "## Jose Pablo Cobos Austria  A01274631 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_rl1BhradrK"
      },
      "source": [
        "### 1.- Descripcion de la actividad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLNhxwR5bx44"
      },
      "source": [
        "Este documento trata sobre el uso de la herramienta PySpark para el manejo de un conjunto de datos de gran volumen, para poder generar un modelo inteligente,y asimismo hacer uso de la aplicacion de Tableau para poder visualizar los datos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rJgQgKgbzMt"
      },
      "source": [
        "### 2.- Configuracion del entorno de trabajo para usar PySpark "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dfU6EhnFaot"
      },
      "source": [
        "Lo primero que vamos hacer antes de iniciar con nuestra generación el modelo es la configuración del entorno que utilizaremos para trabajar con PySpark. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En cuánto a la información o características del sistema que podemos trabajar tenemos que estamos trabajando con el sistema operativo Arch Linux v6.0.9, un procesador Intel i5-10500H y un total de 16 gigas de RAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKSvLXn5GThz"
      },
      "source": [
        "A continuación importamos lo que son las todas las librerías Spark que utilizaremos para poder trabajar con los datos y generar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Lo siguiente que se realizó fue iniciar una sesión de Spark qué será nuestro entorno de trabajo, bajo el nombre de Cobos Big Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2t3EnIzXFSB6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/11/26 09:43:15 WARN Utils: Your hostname, CubesLaptop resolves to a loopback address: 127.0.1.1; using 10.25.65.197 instead (on interface wlp0s20f3)\n",
            "22/11/26 09:43:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/11/26 09:43:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Cobos Big Data\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nos aseguramos que nuestra configuracion fuese la correcta y estuviera funcionando "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "RZpf31SPFZ9S",
        "outputId": "7a5c6825-4b4b-49dc-acf1-47788f7ba634"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://10.25.65.197:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Cobos Big Data</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7e4d480d30>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk0Yyvt8cAvE"
      },
      "source": [
        "### 3.- Seleccion y carga de datos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HkGxkurBcVP"
      },
      "source": [
        "Ya con nuestro entorno de trabajo ya configurado correctamente el siguiente paso a seguir fue la selección y carga de datos, utilizará fue encontrado del siguiente link: https://bit.ly/3EzU4ai\n",
        "\n",
        "El peso total de dicho es más de  1 GB, por lo que cumple con el objetivo de analizar un dataset con un gran volumen de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CECfitKSGSMY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_main = spark.read.option(\"inferSchema\", \"true\").csv(\"geometries.csv\",header=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64gQ3R8ZchXT"
      },
      "source": [
        "### 4.- Creacion del modelo inteligente usando MLib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3DPDOlYg5WT"
      },
      "source": [
        "##### 4.1 EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para la generacion del modelo inteligente lo primero que vamos a realizar es el EDA (Exploratory Data Analysis), para poder visualizar la informacion que tenemos de nuestros datos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imprimimos un esquema de nuestro dataset, para poder visualizar cuales son las columnas y que tipo de dato contienen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "j0xa4BElch8B",
        "outputId": "9e15f571-b175-4875-877c-d53e6135db10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- apartment_id: string (nullable = true)\n",
            " |-- site_id: integer (nullable = true)\n",
            " |-- building_id: integer (nullable = true)\n",
            " |-- plan_id: integer (nullable = true)\n",
            " |-- floor_id: integer (nullable = true)\n",
            " |-- unit_id: integer (nullable = true)\n",
            " |-- area_id: string (nullable = true)\n",
            " |-- unit_usage: string (nullable = true)\n",
            " |-- entity_type: string (nullable = true)\n",
            " |-- entity_subtype: string (nullable = true)\n",
            " |-- geometry: string (nullable = true)\n",
            " |-- elevation: double (nullable = true)\n",
            " |-- height: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_main.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "|        apartment_id|site_id|building_id|plan_id|floor_id|unit_id| area_id|unit_usage|entity_type|entity_subtype|            geometry|elevation|height|\n",
            "+--------------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|665573.0|COMMERCIAL|       area|         LOBBY|POLYGON ((1.60122...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368602.0|COMMERCIAL|       area|     STOREROOM|POLYGON ((1.81678...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368605.0|COMMERCIAL|       area|       BALCONY|POLYGON ((-5.8129...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368617.0|COMMERCIAL|       area|SANITARY_ROOMS|POLYGON ((0.58635...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368608.0|COMMERCIAL|       area|     CLOAKROOM|POLYGON ((-3.6615...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368621.0|COMMERCIAL|       area|       BALCONY|POLYGON ((5.24968...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368609.0|COMMERCIAL|       area|SANITARY_ROOMS|POLYGON ((3.13988...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368618.0|COMMERCIAL|       area|SANITARY_ROOMS|POLYGON ((1.23153...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368615.0|COMMERCIAL|       area|  SPORTS_ROOMS|POLYGON ((-5.6221...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368620.0|COMMERCIAL|       area|COMMON_KITCHEN|POLYGON ((3.85287...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368606.0|COMMERCIAL|       area|SANITARY_ROOMS|POLYGON ((0.43699...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|665573.0|COMMERCIAL|    feature|        STAIRS|POLYGON ((1.14388...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368617.0|COMMERCIAL|    feature|       BATHTUB|POLYGON ((1.58325...|     14.5|   0.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368617.0|COMMERCIAL|    feature|          SINK|POLYGON ((-0.8949...|     15.1|  0.25|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368609.0|COMMERCIAL|    feature|        TOILET|POLYGON ((3.07107...|     14.5|   0.4|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368609.0|COMMERCIAL|    feature|          SINK|POLYGON ((2.57383...|     15.1|  0.25|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368620.0|COMMERCIAL|    feature|       KITCHEN|POLYGON ((2.12277...|     14.5|   0.9|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368620.0|COMMERCIAL|    feature|       KITCHEN|POLYGON ((3.82298...|     14.5|   0.9|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368606.0|COMMERCIAL|    feature|        TOILET|POLYGON ((0.74000...|     14.5|   0.4|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|     nan|COMMERCIAL|  separator|          WALL|POLYGON ((1.13543...|     14.5|   2.6|\n",
            "+--------------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_main.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos observar arriba el dataset anterior y lo que se vio en el el sitio web del dataset de Kaggle este dataset es de un conjunto de apartamentos, teniendo informacion de un conjunto de mas de 42,500 departamentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por eso para este caso, se realizara una realizara un regresion logistica para saber si unit_sage sera Residencial o Publica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neMPhXTshBJe"
      },
      "source": [
        "##### 4.2 ETL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS0TA9MQhLyZ"
      },
      "source": [
        "Primero vamos a quitar todas las filas que tienen Comercial en la columna de unit_sage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_main_filtrado = df_main.where(df_main.unit_usage!=\"COMMERCIAL\")\n",
        "\n",
        "df_main_filtrado = df_main_filtrado.where(df_main.unit_usage!=\"JANITOR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "|apartment_id|site_id|building_id|plan_id|floor_id|unit_id| area_id|unit_usage|entity_type|entity_subtype|            geometry|elevation|height|\n",
            "+------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "|        null|    692|       1460|   4185|    6365|   null|368603.0|    PUBLIC|       area|     STAIRCASE|POLYGON ((4.41013...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368604.0|    PUBLIC|       area|     STAIRCASE|POLYGON ((2.16726...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368616.0|    PUBLIC|       area|      ELEVATOR|POLYGON ((4.03602...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|918411.0|    PUBLIC|       area|         SHAFT|POLYGON ((3.88863...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368613.0|    PUBLIC|       area|         SHAFT|POLYGON ((0.10086...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368603.0|    PUBLIC|    feature|        STAIRS|POLYGON ((5.81215...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368603.0|    PUBLIC|    feature|        STAIRS|POLYGON ((6.13714...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368603.0|    PUBLIC|    feature|        STAIRS|POLYGON ((5.09332...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368604.0|    PUBLIC|    feature|        STAIRS|POLYGON ((2.77267...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368604.0|    PUBLIC|    feature|        STAIRS|POLYGON ((3.59655...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368616.0|    PUBLIC|    feature|      ELEVATOR|POLYGON ((5.19074...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((1.32183...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((3.83479...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((3.89437...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((3.61150...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((-0.2196...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((4.14251...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((4.36043...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((0.26608...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((3.47180...|     14.5|   2.6|\n",
            "+------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_main_filtrado.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuacion buscaremos todos los los valores nulos en el dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 128:=================================================>       (7 + 1) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-------+-----------+-------+--------+-------+-------+----------+-----------+--------------+--------+---------+------+\n",
            "|apartment_id|site_id|building_id|plan_id|floor_id|unit_id|area_id|unit_usage|entity_type|entity_subtype|geometry|elevation|height|\n",
            "+------------+-------+-----------+-------+--------+-------+-------+----------+-----------+--------------+--------+---------+------+\n",
            "|      533477|      0|          0|      0|       0| 533477|2254731|         0|          0|             0|       0|        0|     0|\n",
            "+------------+-------+-----------+-------+--------+-------+-------+----------+-----------+--------------+--------+---------+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "df_main_filtrado.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_main_filtrado.columns]\n",
        "   ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos observar, debido a la gran cantidad de nulls y que no son tan rellevantes para el problema,podemos descartar a las columnas: apartment_id, unit_id, aread_id.\n",
        "\n",
        "Ademas eliminaremos geometry con el simplificar un poco el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_main_filtrado = df_main_filtrado.drop(*('apartment_id','unit_id','area_id','geometry'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "|site_id|building_id|plan_id|floor_id|unit_usage|entity_type|entity_subtype|elevation|height|\n",
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|       area|     STAIRCASE|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|       area|     STAIRCASE|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|       area|      ELEVATOR|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|       area|         SHAFT|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|       area|         SHAFT|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|        STAIRS|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|        STAIRS|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|        STAIRS|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|        STAIRS|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|        STAIRS|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|      ELEVATOR|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_main_filtrado.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Volvemos a buscar valores null "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 84:==================================================>       (7 + 1) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "|site_id|building_id|plan_id|floor_id|unit_usage|entity_type|entity_subtype|elevation|height|\n",
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "|      0|          0|      0|       0|         0|          0|             0|        0|     0|\n",
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_main_filtrado.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_main_filtrado.columns]\n",
        "   ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y como no tenemos 0 ya casi estamos listos para ver la regresion logistica, solo faltaria darle formato a las columnas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer,OneHotEncoder,StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gracias a las herramientas antes importadas, lo que haremos sera formatear todas la variables categoricas que tengamos para que puedan ser usadas en nuestro modelo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "categoricalColumns = ['entity_type','entity_subtype']\n",
        "stages = []\n",
        "for categoricalCol in categoricalColumns:\n",
        "    stringIndexer = StringIndexer(inputCol = categoricalCol,outputCol =  categoricalCol + 'Index')\n",
        "    enconder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols = [categoricalCol + \"classVec\"])\n",
        "    stages += [stringIndexer, enconder]\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol='unit_usage',outputCol= 'label')\n",
        "\n",
        "stages += [label_stringIdx]\n",
        "\n",
        "numericCols = ['site_id','building_id','plan_id','floor_id','elevation','height']\n",
        "\n",
        "assemblerInputs = [c + 'classVec' for c in categoricalColumns] + numericCols\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs,outputCol=\"vectorized_features\")\n",
        "stages += [assembler]\n",
        "scaler = StandardScaler(inputCol= 'vectorized_features',outputCol=\"features\")\n",
        "stages+= [scaler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['label',\n",
              " 'features',\n",
              " 'site_id',\n",
              " 'building_id',\n",
              " 'plan_id',\n",
              " 'floor_id',\n",
              " 'unit_usage',\n",
              " 'entity_type',\n",
              " 'entity_subtype',\n",
              " 'elevation',\n",
              " 'height']"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_main_filtrado.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tambien hacemos uso de pipelines para nuestras nuevas columnas "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "cols = df_main_filtrado.columns\n",
        "pipeline = Pipeline(stages=stages)\n",
        "pipelineModel = pipeline.fit(df_main_filtrado)\n",
        "df_main_filtrado = pipelineModel.transform(df_main_filtrado)\n",
        "selectedCols = ['label','features']+cols\n",
        "df_main_filtrado = df_main_filtrado.select(selectedCols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imprimimos de nuevo el esquema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- label: double (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- site_id: integer (nullable = true)\n",
            " |-- building_id: integer (nullable = true)\n",
            " |-- plan_id: integer (nullable = true)\n",
            " |-- floor_id: integer (nullable = true)\n",
            " |-- unit_usage: string (nullable = true)\n",
            " |-- entity_type: string (nullable = true)\n",
            " |-- entity_subtype: string (nullable = true)\n",
            " |-- elevation: double (nullable = true)\n",
            " |-- height: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_main_filtrado.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUpfshSmhMPx"
      },
      "source": [
        "##### 4.3  Generacion del modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero dividimos todos nuestros datos en parte train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset Count: 2425437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 147:=================================================>       (7 + 1) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Data Count: 606787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "train, test = df_main_filtrado.randomSplit([.8,.2],seed = 2018)\n",
        "print(\"Training Dataset Count: \" + str(train.count()))\n",
        "print(\"Test Data Count: \"+str(test.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 150:=================================================>       (7 + 1) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+\n",
            "| unit_usage|  count|\n",
            "+-----------+-------+\n",
            "|RESIDENTIAL|1998835|\n",
            "|     PUBLIC| 426602|\n",
            "+-----------+-------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "train.groupBy(\"unit_usage\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y aqui es donde vamos a entrenar a nuestro modelo de regresion logistica "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 167:==>              (1 + 3) / 8][Stage 170:======>          (3 + 1) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/11/26 13:07:07 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\n",
            "java.io.IOException: Connecting to /10.25.65.197:39341 failed in the last 4750 ms, fail this connection directly\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
            "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "22/11/26 13:07:07 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\n",
            "java.io.IOException: Connecting to /10.25.65.197:39341 timed out (120000 ms)\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:285)\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
            "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "22/11/26 13:07:07 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\n",
            "java.io.IOException: Connecting to /10.25.65.197:39341 failed in the last 4750 ms, fail this connection directly\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
            "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 167:=======>                                                 (1 + 3) / 8]\r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol= 'features',labelCol='label',maxIter=5)\n",
        "lrModel = lr.fit(train)\n",
        "predictions = lrModel.transform(test)\n",
        "#predictions.select('label','features','rawPrediction','prediction','probability').toPandas().head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOe72x6bciny"
      },
      "source": [
        "### 5.- Evaluacion del modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente para evaludar que el modelo sirva se hace uso de una herramienta conocida como BinaryClassificationEvaluator, que es el area debajo de ROC. RORC es la cuva de probabildiad y AUC es el grado de separabilidad. Esto siginifca que mientras mas alto AUC, mejor es el modelo para distinguir "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "QR8UogKDci8J"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Area Under ROC 0.7503062738527361\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 167:=======>                                                 (1 + 3) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/11/26 13:11:17 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 3 retries)\n",
            "java.io.IOException: Connecting to /10.25.65.197:39341 failed in the last 4750 ms, fail this connection directly\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
            "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "22/11/26 13:11:17 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 3 retries)\n",
            "java.io.IOException: Connecting to /10.25.65.197:39341 failed in the last 4750 ms, fail this connection directly\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
            "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "22/11/26 13:11:17 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 3 retries)\n",
            "java.io.IOException: Connecting to /10.25.65.197:39341 timed out (120000 ms)\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:285)\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
            "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "22/11/26 13:11:17 WARN BlockManager: Failed to fetch remote block taskresult_506 from [BlockManagerId(driver, 10.25.65.197, 39341, None)] after 1 fetch failures. Most recent failure cause:\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
            "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n",
            "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1154)\n",
            "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1098)\n",
            "\tat scala.Option.orElse(Option.scala:447)\n",
            "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1098)\n",
            "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1238)\n",
            "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
            "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: java.io.IOException: Connecting to /10.25.65.197:39341 failed in the last 4750 ms, fail this connection directly\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
            "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\t... 1 more\n",
            "22/11/26 13:11:17 WARN BlockManager: Failed to fetch remote block taskresult_508 from [BlockManagerId(driver, 10.25.65.197, 39341, None)] after 1 fetch failures. Most recent failure cause:\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
            "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n",
            "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1154)\n",
            "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1098)\n",
            "\tat scala.Option.orElse(Option.scala:447)\n",
            "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1098)\n",
            "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1238)\n",
            "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
            "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: java.io.IOException: Connecting to /10.25.65.197:39341 failed in the last 4750 ms, fail this connection directly\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
            "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\t... 1 more\n",
            "22/11/26 13:11:17 WARN BlockManager: Failed to fetch remote block taskresult_507 from [BlockManagerId(driver, 10.25.65.197, 39341, None)] after 1 fetch failures. Most recent failure cause:\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
            "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n",
            "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1154)\n",
            "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1098)\n",
            "\tat scala.Option.orElse(Option.scala:447)\n",
            "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1098)\n",
            "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1238)\n",
            "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
            "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: java.io.IOException: Connecting to /10.25.65.197:39341 timed out (120000 ms)\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:285)\n",
            "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)\n",
            "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:126)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:154)\n",
            "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:184)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\t... 1 more\n",
            "22/11/26 13:11:17 WARN TaskSetManager: Lost task 4.0 in stage 167.0 (TID 506) (10.25.65.197 executor driver): TaskResultLost (result lost from block manager)\n",
            "22/11/26 13:11:17 WARN TaskSetManager: Lost task 5.0 in stage 167.0 (TID 507) (10.25.65.197 executor driver): TaskResultLost (result lost from block manager)\n",
            "22/11/26 13:11:17 WARN TaskSetManager: Lost task 6.0 in stage 167.0 (TID 508) (10.25.65.197 executor driver): TaskResultLost (result lost from block manager)\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "print('Test Area Under ROC',evaluator.evaluate(predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 196:=================================================>       (7 + 1) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy :  0.8445698408172885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
        "print(\"Accuracy : \",accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y finalmente lo probamos la precision del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etzwPyHTdlm3"
      },
      "source": [
        "### 6.- Visualizacion de los datos usando la herramienta de Tableu "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyfGxhQNdl3h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kp4YVRGdmOr"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
