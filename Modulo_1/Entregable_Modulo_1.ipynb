{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heNJLPGoadzh"
      },
      "source": [
        "# Entregable Modulo 1: Utilización, procesamiento y visualización de grandes volúmenes de datos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuAk9Oe0adxq"
      },
      "source": [
        "## Jose Pablo Cobos Austria  A01274631 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_rl1BhradrK"
      },
      "source": [
        "### 1.- Descripcion de la actividad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLNhxwR5bx44"
      },
      "source": [
        "Este documento trata sobre el uso de la herramienta PySpark para el manejo de un conjunto de datos de gran volumen, para poder generar un modelo inteligente,y asimismo hacer uso de la aplicacion de Tableau para poder visualizar los datos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rJgQgKgbzMt"
      },
      "source": [
        "### 2.- Configuracion del entorno de trabajo para usar PySpark "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dfU6EhnFaot"
      },
      "source": [
        "Lo primero que vamos hacer antes de iniciar con nuestra generación el modelo es la configuración del entorno que utilizaremos para trabajar con PySpark. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En cuánto a la información o características del sistema que podemos trabajar tenemos que estamos trabajando con el sistema operativo Arch Linux v6.0.9, un procesador Intel i5-10500H y un total de 16 gigas de RAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKSvLXn5GThz"
      },
      "source": [
        "A continuación importamos lo que son las todas las librerías Spark que utilizaremos para poder trabajar con los datos y generar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Lo siguiente que se realizó fue iniciar una sesión de Spark qué será nuestro entorno de trabajo, bajo el nombre de Cobos Big Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2t3EnIzXFSB6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/11/26 09:43:15 WARN Utils: Your hostname, CubesLaptop resolves to a loopback address: 127.0.1.1; using 10.25.65.197 instead (on interface wlp0s20f3)\n",
            "22/11/26 09:43:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/11/26 09:43:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Cobos Big Data\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nos aseguramos que nuestra configuracion fuese la correcta y estuviera funcionando "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "RZpf31SPFZ9S",
        "outputId": "7a5c6825-4b4b-49dc-acf1-47788f7ba634"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://10.25.65.197:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Cobos Big Data</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7e4d480d30>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk0Yyvt8cAvE"
      },
      "source": [
        "### 3.- Seleccion y carga de datos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HkGxkurBcVP"
      },
      "source": [
        "Ya con nuestro entorno de trabajo ya configurado correctamente el siguiente paso a seguir fue la selección y carga de datos, utilizará fue encontrado del siguiente link: https://bit.ly/3EzU4ai\n",
        "\n",
        "El peso total de dicho es más de  1 GB, por lo que cumple con el objetivo de analizar un dataset con un gran volumen de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CECfitKSGSMY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_main = spark.read.option(\"inferSchema\", \"true\").csv(\"geometries.csv\",header=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64gQ3R8ZchXT"
      },
      "source": [
        "### 4.- Creacion del modelo inteligente usando MLib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3DPDOlYg5WT"
      },
      "source": [
        "##### 4.1 EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para la generacion del modelo inteligente lo primero que vamos a realizar es el EDA (Exploratory Data Analysis), para poder visualizar la informacion que tenemos de nuestros datos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imprimimos un esquema de nuestro dataset, para poder visualizar cuales son las columnas y que tipo de dato contienen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "j0xa4BElch8B",
        "outputId": "9e15f571-b175-4875-877c-d53e6135db10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- apartment_id: string (nullable = true)\n",
            " |-- site_id: integer (nullable = true)\n",
            " |-- building_id: integer (nullable = true)\n",
            " |-- plan_id: integer (nullable = true)\n",
            " |-- floor_id: integer (nullable = true)\n",
            " |-- unit_id: integer (nullable = true)\n",
            " |-- area_id: string (nullable = true)\n",
            " |-- unit_usage: string (nullable = true)\n",
            " |-- entity_type: string (nullable = true)\n",
            " |-- entity_subtype: string (nullable = true)\n",
            " |-- geometry: string (nullable = true)\n",
            " |-- elevation: double (nullable = true)\n",
            " |-- height: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_main.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "|        apartment_id|site_id|building_id|plan_id|floor_id|unit_id| area_id|unit_usage|entity_type|entity_subtype|            geometry|elevation|height|\n",
            "+--------------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|665573.0|COMMERCIAL|       area|         LOBBY|POLYGON ((1.60122...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368602.0|COMMERCIAL|       area|     STOREROOM|POLYGON ((1.81678...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368605.0|COMMERCIAL|       area|       BALCONY|POLYGON ((-5.8129...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368617.0|COMMERCIAL|       area|SANITARY_ROOMS|POLYGON ((0.58635...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368608.0|COMMERCIAL|       area|     CLOAKROOM|POLYGON ((-3.6615...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368621.0|COMMERCIAL|       area|       BALCONY|POLYGON ((5.24968...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368609.0|COMMERCIAL|       area|SANITARY_ROOMS|POLYGON ((3.13988...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368618.0|COMMERCIAL|       area|SANITARY_ROOMS|POLYGON ((1.23153...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368615.0|COMMERCIAL|       area|  SPORTS_ROOMS|POLYGON ((-5.6221...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368620.0|COMMERCIAL|       area|COMMON_KITCHEN|POLYGON ((3.85287...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368606.0|COMMERCIAL|       area|SANITARY_ROOMS|POLYGON ((0.43699...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|665573.0|COMMERCIAL|    feature|        STAIRS|POLYGON ((1.14388...|     14.5|   2.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368617.0|COMMERCIAL|    feature|       BATHTUB|POLYGON ((1.58325...|     14.5|   0.6|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368617.0|COMMERCIAL|    feature|          SINK|POLYGON ((-0.8949...|     15.1|  0.25|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368609.0|COMMERCIAL|    feature|        TOILET|POLYGON ((3.07107...|     14.5|   0.4|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368609.0|COMMERCIAL|    feature|          SINK|POLYGON ((2.57383...|     15.1|  0.25|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368620.0|COMMERCIAL|    feature|       KITCHEN|POLYGON ((2.12277...|     14.5|   0.9|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368620.0|COMMERCIAL|    feature|       KITCHEN|POLYGON ((3.82298...|     14.5|   0.9|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|368606.0|COMMERCIAL|    feature|        TOILET|POLYGON ((0.74000...|     14.5|   0.4|\n",
            "|d338ccd5607781e63...|    692|       1460|   4185|    6365|  40063|     nan|COMMERCIAL|  separator|          WALL|POLYGON ((1.13543...|     14.5|   2.6|\n",
            "+--------------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_main.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos observar arriba el dataset anterior y lo que se vio en el el sitio web del dataset de Kaggle este dataset es de un conjunto de apartamentos, teniendo informacion de un conjunto de mas de 42,500 departamentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por eso para este caso, se realizara una realizara un regresion logistica para saber si unit_sage sera Residencial o Publica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neMPhXTshBJe"
      },
      "source": [
        "##### 4.2 ETL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS0TA9MQhLyZ"
      },
      "source": [
        "Primero vamos a quitar todas las filas que tienen Comercial en la columna de unit_sage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_main_filtrado = df_main.where(df_main.unit_usage!=\"COMMERCIAL\")\n",
        "\n",
        "df_main_filtrado = df_main_filtrado.where(df_main.unit_usage!=\"JANITOR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "|apartment_id|site_id|building_id|plan_id|floor_id|unit_id| area_id|unit_usage|entity_type|entity_subtype|            geometry|elevation|height|\n",
            "+------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "|        null|    692|       1460|   4185|    6365|   null|368603.0|    PUBLIC|       area|     STAIRCASE|POLYGON ((4.41013...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368604.0|    PUBLIC|       area|     STAIRCASE|POLYGON ((2.16726...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368616.0|    PUBLIC|       area|      ELEVATOR|POLYGON ((4.03602...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|918411.0|    PUBLIC|       area|         SHAFT|POLYGON ((3.88863...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368613.0|    PUBLIC|       area|         SHAFT|POLYGON ((0.10086...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368603.0|    PUBLIC|    feature|        STAIRS|POLYGON ((5.81215...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368603.0|    PUBLIC|    feature|        STAIRS|POLYGON ((6.13714...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368603.0|    PUBLIC|    feature|        STAIRS|POLYGON ((5.09332...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368604.0|    PUBLIC|    feature|        STAIRS|POLYGON ((2.77267...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368604.0|    PUBLIC|    feature|        STAIRS|POLYGON ((3.59655...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|368616.0|    PUBLIC|    feature|      ELEVATOR|POLYGON ((5.19074...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((1.32183...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((3.83479...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((3.89437...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((3.61150...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((-0.2196...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((4.14251...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((4.36043...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((0.26608...|     14.5|   2.6|\n",
            "|        null|    692|       1460|   4185|    6365|   null|     nan|    PUBLIC|  separator|          WALL|POLYGON ((3.47180...|     14.5|   2.6|\n",
            "+------------+-------+-----------+-------+--------+-------+--------+----------+-----------+--------------+--------------------+---------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_main_filtrado.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuacion buscaremos todos los los valores nulos en el dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 80:==================================================>       (7 + 1) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-------+-----------+-------+--------+-------+-------+----------+-----------+--------------+--------+---------+------+\n",
            "|apartment_id|site_id|building_id|plan_id|floor_id|unit_id|area_id|unit_usage|entity_type|entity_subtype|geometry|elevation|height|\n",
            "+------------+-------+-----------+-------+--------+-------+-------+----------+-----------+--------------+--------+---------+------+\n",
            "|      533477|      0|          0|      0|       0| 533477|2254731|         0|          0|             0|       0|        0|     0|\n",
            "+------------+-------+-----------+-------+--------+-------+-------+----------+-----------+--------------+--------+---------+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "df_main_filtrado.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_main_filtrado.columns]\n",
        "   ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos observar, debido a la gran cantidad de nulls y que no son tan rellevantes para el problema,podemos descartar a las columnas: apartment_id, unit_id, aread_id.\n",
        "\n",
        "Ademas eliminaremos geometry con el simplificar un poco el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_main_filtrado = df_main_filtrado.drop(*('apartment_id','unit_id','area_id','geometry'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "|site_id|building_id|plan_id|floor_id|unit_usage|entity_type|entity_subtype|elevation|height|\n",
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|       area|     STAIRCASE|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|       area|     STAIRCASE|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|       area|      ELEVATOR|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|       area|         SHAFT|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|       area|         SHAFT|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|        STAIRS|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|        STAIRS|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|        STAIRS|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|        STAIRS|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|        STAIRS|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|    feature|      ELEVATOR|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "|    692|       1460|   4185|    6365|    PUBLIC|  separator|          WALL|     14.5|   2.6|\n",
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_main_filtrado.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Volvemos a buscar valores null "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 84:==================================================>       (7 + 1) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "|site_id|building_id|plan_id|floor_id|unit_usage|entity_type|entity_subtype|elevation|height|\n",
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "|      0|          0|      0|       0|         0|          0|             0|        0|     0|\n",
            "+-------+-----------+-------+--------+----------+-----------+--------------+---------+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_main_filtrado.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_main_filtrado.columns]\n",
        "   ).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y como no tenemos 0 ya casi estamos listos para ver la regresion logistica, solo faltaria darle formato a las columnas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer,OneHotEncoder,StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gracias a las herramientas antes importadas, lo que haremos sera formatear todas la variables categoricas que tengamos para que puedan ser usadas en nuestro modelo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "categoricalColumns = ['unit_usage','entity_type','entity_subtype']\n",
        "stages = []\n",
        "for categoricalCol in categoricalColumns:\n",
        "    stringIndexer = StringIndexer(inputCol = categoricalCol,outputCol =  categoricalCol + 'Index')\n",
        "    enconder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols = [categoricalCol + \"classVec\"])\n",
        "    stages += [stringIndexer, enconder]\n",
        "\n",
        "numericCols = ['site_id','building_id','plan_id','floor_id','elevation','height']\n",
        "\n",
        "assemblerInputs = [c + 'classVec' for c in categoricalColumns] + numericCols\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs,outputCol=\"vectorized_features\")\n",
        "stages += [assembler]\n",
        "scaler = StandardScaler(inputCol= 'vectorized_features',outputCol=\"features\")\n",
        "stages+= [scaler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['site_id',\n",
              " 'building_id',\n",
              " 'plan_id',\n",
              " 'floor_id',\n",
              " 'unit_usage',\n",
              " 'entity_type',\n",
              " 'entity_subtype',\n",
              " 'elevation',\n",
              " 'height',\n",
              " 'unit_usageIndex',\n",
              " 'unit_usageclassVec',\n",
              " 'entity_typeIndex',\n",
              " 'entity_typeclassVec',\n",
              " 'entity_subtypeIndex',\n",
              " 'entity_subtypeclassVec',\n",
              " 'vectorized_features',\n",
              " 'features']"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_main_filtrado.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: Output column unit_usageIndex already exists.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [82], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m cols \u001b[39m=\u001b[39m df_main_filtrado\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m      4\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39mstages)\n\u001b[0;32m----> 5\u001b[0m pipelineModel \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mfit(df_main_filtrado)\n\u001b[1;32m      6\u001b[0m df_main_filtrado \u001b[39m=\u001b[39m pipelineModel\u001b[39m.\u001b[39mtransform(df_main_filtrado)\n\u001b[1;32m      7\u001b[0m selectedCols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m+\u001b[39mcols\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[1;32m    135\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m indexOfLastEstimator:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    384\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Output column unit_usageIndex already exists."
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "cols = df_main_filtrado.columns\n",
        "pipeline = Pipeline(stages=stages)\n",
        "pipelineModel = pipeline.fit(df_main_filtrado)\n",
        "df_main_filtrado = pipelineModel.transform(df_main_filtrado)\n",
        "selectedCols = ['label','features']+cols\n",
        "df_main_filtrado = df_main_filtrado.select(selectedCols)\n",
        "df_main_filtrado.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirmamos que nuestros que todos los cambios estuviesen bien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- site_id: integer (nullable = true)\n",
            " |-- building_id: integer (nullable = true)\n",
            " |-- plan_id: integer (nullable = true)\n",
            " |-- floor_id: integer (nullable = true)\n",
            " |-- unit_usage: string (nullable = true)\n",
            " |-- entity_type: string (nullable = true)\n",
            " |-- entity_subtype: string (nullable = true)\n",
            " |-- elevation: double (nullable = true)\n",
            " |-- height: double (nullable = true)\n",
            " |-- unit_usageIndex: double (nullable = false)\n",
            " |-- unit_usageclassVec: vector (nullable = true)\n",
            " |-- entity_typeIndex: double (nullable = false)\n",
            " |-- entity_typeclassVec: vector (nullable = true)\n",
            " |-- entity_subtypeIndex: double (nullable = false)\n",
            " |-- entity_subtypeclassVec: vector (nullable = true)\n",
            " |-- vectorized_features: vector (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_main_filtrado.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUpfshSmhMPx"
      },
      "source": [
        "##### 4.3  Generacion del modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero dividimos todos nuestros datos en parte train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset Count: 2425437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 111:=================================================>       (7 + 1) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Data Count: 606787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "train, test = df_main_filtrado.randomSplit([.8,.2],seed = 2018)\n",
        "print(\"Training Dataset Count: \" + str(train.count()))\n",
        "print(\"Test Data Count: \"+str(test.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 114:=================================================>       (7 + 1) / 8]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+\n",
            "| unit_usage|  count|\n",
            "+-----------+-------+\n",
            "|RESIDENTIAL|1998262|\n",
            "|     PUBLIC| 427175|\n",
            "+-----------+-------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "train.groupBy(\"unit_usage\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "ename": "IllegalArgumentException",
          "evalue": "label does not exist. Available: site_id, building_id, plan_id, floor_id, unit_usage, entity_type, entity_subtype, elevation, height, unit_usageIndex, unit_usageclassVec, entity_typeIndex, entity_typeclassVec, entity_subtypeIndex, entity_subtypeclassVec, vectorized_features, features",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [79], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassification\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      2\u001b[0m lr \u001b[39m=\u001b[39m LogisticRegression(featuresCol\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m,labelCol\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m,maxIter\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m lrModel \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39;49mfit(train)\n\u001b[1;32m      4\u001b[0m predictions \u001b[39m=\u001b[39m lrModel\u001b[39m.\u001b[39mtransform(test)\n\u001b[1;32m      5\u001b[0m predictions\u001b[39m.\u001b[39mselect(\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mrawPrediction\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mprobability\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtoPandas()\u001b[39m.\u001b[39mhead(\u001b[39m5\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    384\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: label does not exist. Available: site_id, building_id, plan_id, floor_id, unit_usage, entity_type, entity_subtype, elevation, height, unit_usageIndex, unit_usageclassVec, entity_typeIndex, entity_typeclassVec, entity_subtypeIndex, entity_subtypeclassVec, vectorized_features, features"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol= 'features',labelCol='label',maxIter=5)\n",
        "lrModel = lr.fit(train)\n",
        "predictions = lrModel.transform(test)\n",
        "predictions.select('label','features','rawPrediction','prediction','probability').toPandas().head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOe72x6bciny"
      },
      "source": [
        "### 5.- Evaluacion del modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR8UogKDci8J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etzwPyHTdlm3"
      },
      "source": [
        "### 6.- Visualizacion de los datos usando la herramienta de Tableu "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyfGxhQNdl3h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kp4YVRGdmOr"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "d2152fd7f0bbc62aa1baff8c990435d1e2c7175d001561303988032604c11a48"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
